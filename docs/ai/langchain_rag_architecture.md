# RAG-–∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ —Å LangChain –∏ LangGraph

## –û–±–∑–æ—Ä RAG —Å–∏—Å—Ç–µ–º—ã

RAG (Retrieval-Augmented Generation) —Å–∏—Å—Ç–µ–º–∞ –ø–æ—Å—Ç—Ä–æ–µ–Ω–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ LangChain –∏ LangGraph, –æ–±—ä–µ–¥–∏–Ω—è—è –ø–æ–∏—Å–∫ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –∏–∑ –∫–æ—Ä–ø–æ—Ä–∞—Ç–∏–≤–Ω–æ–π –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π —Å –≥–µ–Ω–µ—Ä–∞—Ü–∏–µ–π –∫–æ–Ω—Ç–µ–∫—Å—Ç—É–∞–ª—å–Ω—ã—Ö –æ—Ç–≤–µ—Ç–æ–≤ —á–µ—Ä–µ–∑ —É–ø—Ä–∞–≤–ª—è–µ–º—ã–µ —Ü–µ–ø–æ—á–∫–∏ –∏ –≥—Ä–∞—Ñ—ã.

## –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ RAG –ø–∞–π–ø–ª–∞–π–Ω–∞ —Å LangGraph

```mermaid
graph TB
    subgraph "LangGraph Workflow"
        START[Start Node]
        LANG_DETECT[Language Detection Node]
        QUERY_PROCESS[Query Processing Node]
        RETRIEVAL[Retrieval Node]
        RERANK[Reranking Node]
        CONTEXT_BUILD[Context Building Node]
        GENERATION[Generation Node]
        TRANSLATION[Translation Node]
        END[End Node]
    end
    
    subgraph "LangChain Components"
        EMBEDDINGS[LangChain Embeddings]
        VECTORSTORE[LangChain VectorStore]
        RETRIEVER[LangChain Retriever]
        PROMPT_TEMPLATE[LangChain PromptTemplate]
        LLM_CHAIN[LangChain LLM Chain]
        OUTPUT_PARSER[LangChain Output Parser]
    end
    
    subgraph "External Services"
        QDRANT[(Qdrant VectorDB)]
        OPENAI[OpenAI LLM]
        TRANSLATOR[Translation Service]
    end
    
    START --> LANG_DETECT
    LANG_DETECT --> QUERY_PROCESS
    QUERY_PROCESS --> RETRIEVAL
    RETRIEVAL --> RERANK
    RERANK --> CONTEXT_BUILD
    CONTEXT_BUILD --> GENERATION
    GENERATION --> TRANSLATION
    TRANSLATION --> END
    
    RETRIEVAL --> EMBEDDINGS
    EMBEDDINGS --> VECTORSTORE
    VECTORSTORE --> QDRANT
    RETRIEVAL --> RETRIEVER
    
    GENERATION --> PROMPT_TEMPLATE
    PROMPT_TEMPLATE --> LLM_CHAIN
    LLM_CHAIN --> OPENAI
    LLM_CHAIN --> OUTPUT_PARSER
    
    TRANSLATION --> TRANSLATOR
```

## –ö–æ–º–ø–æ–Ω–µ–Ω—Ç—ã RAG —Å–∏—Å—Ç–µ–º—ã —Å LangChain

### 1. Document Processing Pipeline

#### LangChain Text Splitters
```python
from langchain.text_splitter import (
    RecursiveCharacterTextSplitter,
    TokenTextSplitter,
    SemanticChunker
)
from langchain.schema import Document as LangChainDocument
from typing import List, Dict, Any

class LangChainDocumentProcessor:
    def __init__(self):
        # –†–µ–∫—É—Ä—Å–∏–≤–Ω—ã–π —Å–ø–ª–∏—Ç—Ç–µ—Ä –¥–ª—è –æ–±—â–∏—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
        self.recursive_splitter = RecursiveCharacterTextSplitter(
            chunk_size=1000,
            chunk_overlap=200,
            length_function=len,
            separators=["\n\n", "\n", " ", ""]
        )
        
        # –¢–æ–∫–µ–Ω-–±–∞–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Å–ø–ª–∏—Ç—Ç–µ—Ä –¥–ª—è —Ç–æ—á–Ω–æ–≥–æ –∫–æ–Ω—Ç—Ä–æ–ª—è
        self.token_splitter = TokenTextSplitter(
            chunk_size=800,
            chunk_overlap=100
        )
        
        # –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π —Å–ø–ª–∏—Ç—Ç–µ—Ä –¥–ª—è –±–æ–ª–µ–µ —É–º–Ω–æ–≥–æ —Ä–∞–∑–±–∏–µ–Ω–∏—è
        self.semantic_splitter = SemanticChunker(
            embeddings=self.get_embeddings_model(),
            breakpoint_threshold_type="percentile"
        )
    
    async def process_document(
        self, 
        document: Document,
        content: str,
        splitter_type: str = "recursive"
    ) -> List[LangChainDocument]:
        """
        –û–±—Ä–∞–±–æ—Ç–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º LangChain —Å–ø–ª–∏—Ç—Ç–µ—Ä–æ–≤
        """
        # –°–æ–∑–¥–∞–Ω–∏–µ LangChain –¥–æ–∫—É–º–µ–Ω—Ç–∞
        langchain_doc = LangChainDocument(
            page_content=content,
            metadata={
                "document_id": document.id,
                "title": document.title,
                "category": document.category,
                "language": document.language,
                "file_type": document.file_type,
                "uploaded_by": document.uploaded_by
            }
        )
        
        # –í—ã–±–æ—Ä —Å–ø–ª–∏—Ç—Ç–µ—Ä–∞
        if splitter_type == "recursive":
            splitter = self.recursive_splitter
        elif splitter_type == "token":
            splitter = self.token_splitter
        elif splitter_type == "semantic":
            splitter = self.semantic_splitter
        else:
            splitter = self.recursive_splitter
        
        # –†–∞–∑–±–∏–µ–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–∞
        chunks = splitter.split_documents([langchain_doc])
        
        # –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã—Ö –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö –∫ —á–∞–Ω–∫–∞–º
        for i, chunk in enumerate(chunks):
            chunk.metadata.update({
                "chunk_index": i,
                "chunk_id": f"doc_{document.id}_chunk_{i}",
                "total_chunks": len(chunks)
            })
        
        return chunks
```

#### LangChain Embeddings
```python
from langchain.embeddings import (
    OpenAIEmbeddings,
    HuggingFaceEmbeddings,
    CacheBackedEmbeddings
)
from langchain.storage import LocalFileStore
from typing import List, Optional
import os

class LangChainEmbeddingService:
    def __init__(self, model_type: str = "openai"):
        self.model_type = model_type
        
        # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö embedding –º–æ–¥–µ–ª–µ–π
        if model_type == "openai":
            self.base_embeddings = OpenAIEmbeddings(
                model="text-embedding-ada-002",
                openai_api_key=os.getenv("OPENAI_API_KEY")
            )
        elif model_type == "huggingface":
            self.base_embeddings = HuggingFaceEmbeddings(
                model_name="sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2",
                model_kwargs={'device': 'cpu'},
                encode_kwargs={'normalize_embeddings': True}
            )
        
        # –ö—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ embeddings –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏
        self.file_store = LocalFileStore("./storage/embeddings_cache/")
        self.cached_embeddings = CacheBackedEmbeddings.from_bytes_store(
            self.base_embeddings,
            self.file_store,
            namespace=f"embeddings_{model_type}"
        )
    
    async def embed_documents(self, texts: List[str]) -> List[List[float]]:
        """
        –ì–µ–Ω–µ—Ä–∞—Ü–∏—è embeddings –¥–ª—è –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ —Å –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ–º
        """
        return await self.cached_embeddings.aembed_documents(texts)
    
    async def embed_query(self, text: str) -> List[float]:
        """
        –ì–µ–Ω–µ—Ä–∞—Ü–∏—è embedding –¥–ª—è –ø–æ–∏—Å–∫–æ–≤–æ–≥–æ –∑–∞–ø—Ä–æ—Å–∞
        """
        return await self.cached_embeddings.aembed_query(text)
    
    def get_embeddings_model(self):
        """
        –ü–æ–ª—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ embeddings –¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –≤ –¥—Ä—É–≥–∏—Ö –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–∞—Ö
        """
        return self.cached_embeddings
```

### 2. LangChain Vector Store Integration

#### Qdrant Vector Store
```python
from langchain.vectorstores import Qdrant
from langchain.schema import Document as LangChainDocument
from qdrant_client import QdrantClient
from qdrant_client.models import Distance, VectorParams
from typing import List, Dict, Any, Optional

class LangChainVectorStore:
    def __init__(
        self, 
        embeddings_service: LangChainEmbeddingService,
        qdrant_url: str = "http://localhost:6333",
        collection_name: str = "documents"
    ):
        self.embeddings = embeddings_service.get_embeddings_model()
        self.collection_name = collection_name
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è Qdrant –∫–ª–∏–µ–Ω—Ç–∞
        self.qdrant_client = QdrantClient(url=qdrant_url)
        
        # –°–æ–∑–¥–∞–Ω–∏–µ LangChain Qdrant –≤–µ–∫—Ç–æ—Ä–Ω–æ–≥–æ —Ö—Ä–∞–Ω–∏–ª–∏—â–∞
        self.vector_store = Qdrant(
            client=self.qdrant_client,
            collection_name=collection_name,
            embeddings=self.embeddings
        )
    
    async def initialize_collection(self, vector_size: int = 1536):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∫–æ–ª–ª–µ–∫—Ü–∏–∏ –≤ Qdrant
        """
        try:
            self.qdrant_client.create_collection(
                collection_name=self.collection_name,
                vectors_config=VectorParams(
                    size=vector_size,
                    distance=Distance.COSINE
                )
            )
        except Exception:
            # –ö–æ–ª–ª–µ–∫—Ü–∏—è —É–∂–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç
            pass
    
    async def add_documents(
        self, 
        documents: List[LangChainDocument],
        ids: Optional[List[str]] = None
    ) -> List[str]:
        """
        –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –≤ –≤–µ–∫—Ç–æ—Ä–Ω–æ–µ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ
        """
        return await self.vector_store.aadd_documents(
            documents=documents,
            ids=ids
        )
    
    async def similarity_search_with_score(
        self,
        query: str,
        k: int = 10,
        filter: Optional[Dict[str, Any]] = None,
        score_threshold: Optional[float] = None
    ) -> List[tuple[LangChainDocument, float]]:
        """
        –ü–æ–∏—Å–∫ –ø–æ—Ö–æ–∂–∏—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ —Å –æ—Ü–µ–Ω–∫–∞–º–∏ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏
        """
        return await self.vector_store.asimilarity_search_with_score(
            query=query,
            k=k,
            filter=filter,
            score_threshold=score_threshold
        )
    
    def as_retriever(
        self,
        search_type: str = "similarity",
        search_kwargs: Optional[Dict[str, Any]] = None
    ):
        """
        –°–æ–∑–¥–∞–Ω–∏–µ LangChain Retriever –∏–∑ –≤–µ–∫—Ç–æ—Ä–Ω–æ–≥–æ —Ö—Ä–∞–Ω–∏–ª–∏—â–∞
        """
        return self.vector_store.as_retriever(
            search_type=search_type,
            search_kwargs=search_kwargs or {"k": 10}
        )
```

### 3. LangGraph Workflow for Query Processing

#### LangGraph State and Nodes
```python
from langgraph.graph import StateGraph, END
from langchain.schema import BaseMessage
from typing import TypedDict, List, Dict, Any
import operator

class RAGState(TypedDict):
    """–°–æ—Å—Ç–æ—è–Ω–∏–µ RAG workflow"""
    query: str
    user_id: int
    language: str
    processed_query: str
    retrieved_docs: List[LangChainDocument]
    context: str
    response: str
    metadata: Dict[str, Any]
    messages: List[BaseMessage]

class LangGraphRAGWorkflow:
    def __init__(
        self,
        embeddings_service: LangChainEmbeddingService,
        vector_store: LangChainVectorStore,
        llm_service
    ):
        self.embeddings = embeddings_service
        self.vector_store = vector_store
        self.llm = llm_service
        
        # –°–æ–∑–¥–∞–Ω–∏–µ –≥—Ä–∞—Ñ–∞
        self.workflow = StateGraph(RAGState)
        self._build_workflow()
    
    def _build_workflow(self):
        """–ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ LangGraph workflow"""
        
        # –î–æ–±–∞–≤–ª–µ–Ω–∏–µ —É–∑–ª–æ–≤
        self.workflow.add_node("detect_language", self.detect_language_node)
        self.workflow.add_node("process_query", self.process_query_node)
        self.workflow.add_node("retrieve_documents", self.retrieve_documents_node)
        self.workflow.add_node("rerank_documents", self.rerank_documents_node)
        self.workflow.add_node("build_context", self.build_context_node)
        self.workflow.add_node("generate_response", self.generate_response_node)
        self.workflow.add_node("translate_response", self.translate_response_node)
        
        # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –º–∞—Ä—à—Ä—É—Ç–æ–≤
        self.workflow.set_entry_point("detect_language")
        
        self.workflow.add_edge("detect_language", "process_query")
        self.workflow.add_edge("process_query", "retrieve_documents")
        self.workflow.add_edge("retrieve_documents", "rerank_documents")
        self.workflow.add_edge("rerank_documents", "build_context")
        self.workflow.add_edge("build_context", "generate_response")
        
        # –£—Å–ª–æ–≤–Ω—ã–π –º–∞—Ä—à—Ä—É—Ç –¥–ª—è –ø–µ—Ä–µ–≤–æ–¥–∞
        self.workflow.add_conditional_edges(
            "generate_response",
            self.should_translate,
            {
                "translate": "translate_response",
                "end": END
            }
        )
        
        self.workflow.add_edge("translate_response", END)
        
        # –ö–æ–º–ø–∏–ª—è—Ü–∏—è –≥—Ä–∞—Ñ–∞
        self.app = self.workflow.compile()
    
    async def detect_language_node(self, state: RAGState) -> RAGState:
        """–£–∑–µ–ª –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —è–∑—ã–∫–∞"""
        from langdetect import detect
        
        try:
            detected_language = detect(state["query"])
            state["language"] = detected_language
        except:
            state["language"] = "ru"  # Fallback
        
        state["metadata"]["language_detected"] = state["language"]
        return state
    
    async def process_query_node(self, state: RAGState) -> RAGState:
        """–£–∑–µ–ª –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∑–∞–ø—Ä–æ—Å–∞"""
        from langchain.prompts import PromptTemplate
        from langchain.chains import LLMChain
        
        # –®–∞–±–ª–æ–Ω –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –∑–∞–ø—Ä–æ—Å–∞
        query_enhancement_template = PromptTemplate(
            input_variables=["query", "language"],
            template="""
            Improve the following search query for better document retrieval.
            Add relevant synonyms and related terms while keeping the original meaning.
            
            Original query ({language}): {query}
            
            Enhanced query:
            """
        )
        
        chain = LLMChain(llm=self.llm, prompt=query_enhancement_template)
        
        enhanced_query = await chain.arun(
            query=state["query"],
            language=state["language"]
        )
        
        state["processed_query"] = enhanced_query.strip()
        return state
    
    async def retrieve_documents_node(self, state: RAGState) -> RAGState:
        """–£–∑–µ–ª –ø–æ–∏—Å–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤"""
        retriever = self.vector_store.as_retriever(
            search_type="similarity_score_threshold",
            search_kwargs={
                "k": 20,
                "score_threshold": 0.7
            }
        )
        
        retrieved_docs = await retriever.aget_relevant_documents(
            state["processed_query"]
        )
        
        state["retrieved_docs"] = retrieved_docs
        state["metadata"]["retrieved_count"] = len(retrieved_docs)
        return state
    
    async def rerank_documents_node(self, state: RAGState) -> RAGState:
        """–£–∑–µ–ª –ø–µ—Ä–µ—Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏—è –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤"""
        from langchain.document_transformers import LongContextReorder
        
        # –ü–µ—Ä–µ—Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–ª—è –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–≥–æ –ø–æ—Ä—è–¥–∫–∞
        reorderer = LongContextReorder()
        reranked_docs = reorderer.transform_documents(state["retrieved_docs"])
        
        # –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
        state["retrieved_docs"] = reranked_docs[:10]
        return state
    
    async def build_context_node(self, state: RAGState) -> RAGState:
        """–£–∑–µ–ª –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞"""
        context_parts = []
        
        for i, doc in enumerate(state["retrieved_docs"]):
            context_parts.append(
                f"–î–æ–∫—É–º–µ–Ω—Ç {i+1}:\n{doc.page_content}\n"
                f"–ò—Å—Ç–æ—á–Ω–∏–∫: {doc.metadata.get('title', '–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–æ')}\n"
            )
        
        state["context"] = "\n".join(context_parts)
        state["metadata"]["context_length"] = len(state["context"])
        return state
    
    async def generate_response_node(self, state: RAGState) -> RAGState:
        """–£–∑–µ–ª –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç–≤–µ—Ç–∞"""
        from langchain.prompts import PromptTemplate
        from langchain.chains import LLMChain
        
        response_template = PromptTemplate(
            input_variables=["query", "context", "language"],
            template="""
            –¢—ã - –∫–æ—Ä–ø–æ—Ä–∞—Ç–∏–≤–Ω—ã–π –ø–æ–º–æ—â–Ω–∏–∫ –¥–ª—è –æ–Ω–±–æ—Ä–¥–∏–Ω–≥–∞ —Å–æ—Ç—Ä—É–¥–Ω–∏–∫–æ–≤.
            –ò—Å–ø–æ–ª—å–∑—É–π –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç –¥–ª—è –æ—Ç–≤–µ—Ç–∞ –Ω–∞ –≤–æ–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è.
            
            –ö–æ–Ω—Ç–µ–∫—Å—Ç:
            {context}
            
            –í–æ–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è: {query}
            –Ø–∑—ã–∫ –æ—Ç–≤–µ—Ç–∞: {language}
            
            –ò–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏:
            - –û—Ç–≤–µ—á–∞–π –Ω–∞ —è–∑—ã–∫–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è ({language})
            - –ë—É–¥—å –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–º –∏ –ø–æ–ª–µ–∑–Ω—ã–º
            - –ï—Å–ª–∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –Ω–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ, —Å–∫–∞–∂–∏ –æ–± —ç—Ç–æ–º —á–µ—Å—Ç–Ω–æ
            - –°—Å—ã–ª–∞–π—Å—è –Ω–∞ –∏—Å—Ç–æ—á–Ω–∏–∫–∏ –∏–∑ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
            - –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–π –¥—Ä—É–∂–µ–ª—é–±–Ω—ã–π –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—ã–π —Ç–æ–Ω
            
            –û—Ç–≤–µ—Ç:
            """
        )
        
        chain = LLMChain(llm=self.llm, prompt=response_template)
        
        response = await chain.arun(
            query=state["query"],
            context=state["context"],
            language=state["language"]
        )
        
        state["response"] = response.strip()
        return state
    
    async def translate_response_node(self, state: RAGState) -> RAGState:
        """–£–∑–µ–ª –ø–µ—Ä–µ–≤–æ–¥–∞ –æ—Ç–≤–µ—Ç–∞"""
        # –ó–¥–µ—Å—å –º–æ–∂–Ω–æ –¥–æ–±–∞–≤–∏—Ç—å –ª–æ–≥–∏–∫—É –ø–µ—Ä–µ–≤–æ–¥–∞ –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
        # –ü–æ–∫–∞ –ø—Ä–æ—Å—Ç–æ –≤–æ–∑–≤—Ä–∞—â–∞–µ–º —Å–æ—Å—Ç–æ—è–Ω–∏–µ –±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π
        return state
    
    def should_translate(self, state: RAGState) -> str:
        """–£—Å–ª–æ–≤–∏–µ –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏ –ø–µ—Ä–µ–≤–æ–¥–∞"""
        # –ï—Å–ª–∏ —è–∑—ã–∫ –Ω–µ —Ä—É—Å—Å–∫–∏–π –∏ –Ω–µ –∞–Ω–≥–ª–∏–π—Å–∫–∏–π, –ø–µ—Ä–µ–≤–æ–¥–∏–º
        if state["language"] not in ["ru", "en"]:
            return "translate"
        return "end"
    
    async def process_query(
        self,
        query: str,
        user_id: int,
        metadata: Optional[Dict[str, Any]] = None
    ) -> Dict[str, Any]:
        """
        –û—Å–Ω–æ–≤–Ω–æ–π –º–µ—Ç–æ–¥ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∑–∞–ø—Ä–æ—Å–∞ —á–µ—Ä–µ–∑ LangGraph
        """
        initial_state = RAGState(
            query=query,
            user_id=user_id,
            language="",
            processed_query="",
            retrieved_docs=[],
            context="",
            response="",
            metadata=metadata or {},
            messages=[]
        )
        
        # –í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ workflow
        final_state = await self.app.ainvoke(initial_state)
        
        return {
            "response": final_state["response"],
            "context": final_state["context"],
            "language": final_state["language"],
            "retrieved_docs_count": len(final_state["retrieved_docs"]),
            "metadata": final_state["metadata"]
        }
```

### 4. Advanced LangChain Chains

#### Multi-Step Reasoning Chain
```python
from langchain.chains import SequentialChain, LLMChain
from langchain.prompts import PromptTemplate
from langchain.output_parsers import PydanticOutputParser
from pydantic import BaseModel, Field
from typing import List

class ReasoningStep(BaseModel):
    step: int = Field(description="–ù–æ–º–µ—Ä —à–∞–≥–∞ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è")
    thought: str = Field(description="–ú—ã—Å–ª—å –∏–ª–∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–µ –Ω–∞ —ç—Ç–æ–º —à–∞–≥–µ")
    action: str = Field(description="–î–µ–π—Å—Ç–≤–∏–µ, –∫–æ—Ç–æ—Ä–æ–µ –Ω—É–∂–Ω–æ –ø—Ä–µ–¥–ø—Ä–∏–Ω—è—Ç—å")
    result: str = Field(description="–†–µ–∑—É–ª—å—Ç–∞—Ç –¥–µ–π—Å—Ç–≤–∏—è")

class ReasoningChain(BaseModel):
    steps: List[ReasoningStep] = Field(description="–®–∞–≥–∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è")
    final_answer: str = Field(description="–§–∏–Ω–∞–ª—å–Ω—ã–π –æ—Ç–≤–µ—Ç")

class MultiStepReasoningChain:
    def __init__(self, llm):
        self.llm = llm
        self.parser = PydanticOutputParser(pydantic_object=ReasoningChain)
        
        # –®–∞–±–ª–æ–Ω –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –∑–∞–ø—Ä–æ—Å–∞
        self.analysis_template = PromptTemplate(
            input_variables=["query", "context"],
            template="""
            –ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π —Å–ª–µ–¥—É—é—â–∏–π –∑–∞–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç.
            –û–ø—Ä–µ–¥–µ–ª–∏, —Ç—Ä–µ–±—É–µ—Ç –ª–∏ –∑–∞–ø—Ä–æ—Å –º–Ω–æ–≥–æ—à–∞–≥–æ–≤–æ–≥–æ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è.
            
            –ó–∞–ø—Ä–æ—Å: {query}
            –ö–æ–Ω—Ç–µ–∫—Å—Ç: {context}
            
            –ï—Å–ª–∏ –∑–∞–ø—Ä–æ—Å —Å–ª–æ–∂–Ω—ã–π, —Ä–∞–∑–±–µ–π –µ–≥–æ –Ω–∞ –ª–æ–≥–∏—á–µ—Å–∫–∏–µ —à–∞–≥–∏.
            –ï—Å–ª–∏ –ø—Ä–æ—Å—Ç–æ–π, –¥–∞–π –ø—Ä—è–º–æ–π –æ—Ç–≤–µ—Ç.
            
            {format_instructions}
            """,
            partial_variables={"format_instructions": self.parser.get_format_instructions()}
        )
        
        # –¶–µ–ø–æ—á–∫–∞ –∞–Ω–∞–ª–∏–∑–∞
        self.analysis_chain = LLMChain(
            llm=self.llm,
            prompt=self.analysis_template,
            output_parser=self.parser
        )
    
    async def process_complex_query(
        self,
        query: str,
        context: str
    ) -> ReasoningChain:
        """
        –û–±—Ä–∞–±–æ—Ç–∫–∞ —Å–ª–æ–∂–Ω–æ–≥–æ –∑–∞–ø—Ä–æ—Å–∞ —Å –º–Ω–æ–≥–æ—à–∞–≥–æ–≤—ã–º —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–µ–º
        """
        reasoning = await self.analysis_chain.arun(
            query=query,
            context=context
        )
        
        return reasoning
```

#### Conversational RAG Chain
```python
from langchain.chains import ConversationalRetrievalChain
from langchain.memory import ConversationBufferWindowMemory
from langchain.schema import BaseMessage

class ConversationalRAGService:
    def __init__(
        self,
        llm,
        vector_store: LangChainVectorStore,
        memory_window: int = 10
    ):
        self.llm = llm
        self.vector_store = vector_store
        
        # –ü–∞–º—è—Ç—å –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è –∏—Å—Ç–æ—Ä–∏–∏ –¥–∏–∞–ª–æ–≥–∞
        self.memory = ConversationBufferWindowMemory(
            k=memory_window,
            memory_key="chat_history",
            return_messages=True,
            output_key="answer"
        )
        
        # –°–æ–∑–¥–∞–Ω–∏–µ conversational retrieval chain
        self.chain = ConversationalRetrievalChain.from_llm(
            llm=self.llm,
            retriever=self.vector_store.as_retriever(),
            memory=self.memory,
            return_source_documents=True,
            verbose=True
        )
    
    async def chat(
        self,
        query: str,
        user_id: int
    ) -> Dict[str, Any]:
        """
        –î–∏–∞–ª–æ–≥–æ–≤—ã–π –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å —Å —É—á–µ—Ç–æ–º –∏—Å—Ç–æ—Ä–∏–∏
        """
        result = await self.chain.acall({
            "question": query,
            "chat_history": self.memory.chat_memory.messages
        })
        
        return {
            "answer": result["answer"],
            "source_documents": [
                {
                    "content": doc.page_content,
                    "metadata": doc.metadata
                }
                for doc in result["source_documents"]
            ],
            "chat_history": [
                {
                    "type": msg.type,
                    "content": msg.content
                }
                for msg in self.memory.chat_memory.messages
            ]
        }
    
    def clear_memory(self):
        """–û—á–∏—Å—Ç–∫–∞ –ø–∞–º—è—Ç–∏ –¥–∏–∞–ª–æ–≥–∞"""
        self.memory.clear()
```

### 5. LangChain Tools Integration

#### Custom Tools for Corporate Functions
```python
from langchain.tools import BaseTool
from typing import Optional, Type
from pydantic import BaseModel, Field

class UserSearchInput(BaseModel):
    query: str = Field(description="–ü–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å –¥–ª—è –ø–æ–∏—Å–∫–∞ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π")
    department: Optional[str] = Field(description="–î–µ–ø–∞—Ä—Ç–∞–º–µ–Ω—Ç –¥–ª—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏")

class UserSearchTool(BaseTool):
    name = "user_search"
    description = "–ü–æ–∏—Å–∫ —Å–æ—Ç—Ä—É–¥–Ω–∏–∫–æ–≤ –ø–æ –∏–º–µ–Ω–∏, –¥–µ–ø–∞—Ä—Ç–∞–º–µ–Ω—Ç—É –∏–ª–∏ –¥–æ–ª–∂–Ω–æ—Å—Ç–∏"
    args_schema: Type[BaseModel] = UserSearchInput
    
    def __init__(self, user_service):
        super().__init__()
        self.user_service = user_service
    
    async def _arun(
        self,
        query: str,
        department: Optional[str] = None
    ) -> str:
        """–ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π –ø–æ–∏—Å–∫ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π"""
        users = await self.user_service.search_users(
            query=query,
            department=department
        )
        
        if not users:
            return "–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã"
        
        result = "–ù–∞–π–¥–µ–Ω–Ω—ã–µ —Å–æ—Ç—Ä—É–¥–Ω–∏–∫–∏:\n"
        for user in users[:5]:  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–æ 5 —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
            result += f"- {user.first_name} {user.last_name} ({user.department})\n"
        
        return result

class DocumentSearchInput(BaseModel):
    query: str = Field(description="–ü–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å –¥–ª—è –ø–æ–∏—Å–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤")
    category: Optional[str] = Field(description="–ö–∞—Ç–µ–≥–æ—Ä–∏—è –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤")

class DocumentSearchTool(BaseTool):
    name = "document_search"
    description = "–ü–æ–∏—Å–∫ –∫–æ—Ä–ø–æ—Ä–∞—Ç–∏–≤–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –ø–æ —Å–æ–¥–µ—Ä–∂–∏–º–æ–º—É –∏–ª–∏ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º"
    args_schema: Type[BaseModel] = DocumentSearchInput
    
    def __init__(self, document_service):
        super().__init__()
        self.document_service = document_service
    
    async def _arun(
        self,
        query: str,
        category: Optional[str] = None
    ) -> str:
        """–ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π –ø–æ–∏—Å–∫ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤"""
        documents = await self.document_service.search_documents(
            query=query,
            category=category
        )
        
        if not documents:
            return "–î–æ–∫—É–º–µ–Ω—Ç—ã –Ω–µ –Ω–∞–π–¥–µ–Ω—ã"
        
        result = "–ù–∞–π–¥–µ–Ω–Ω—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã:\n"
        for doc in documents[:3]:  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–æ 3 —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
            result += f"- {doc.title} ({doc.category})\n"
        
        return result
```

#### Agent with Tools
```python
from langchain.agents import AgentExecutor, create_openai_tools_agent
from langchain.prompts import ChatPromptTemplate

class CorporateAssistantAgent:
    def __init__(
        self,
        llm,
        user_service,
        document_service,
        vector_store: LangChainVectorStore
    ):
        self.llm = llm
        
        # –°–æ–∑–¥–∞–Ω–∏–µ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤
        self.tools = [
            UserSearchTool(user_service),
            DocumentSearchTool(document_service),
            # –ú–æ–∂–Ω–æ –¥–æ–±–∞–≤–∏—Ç—å –±–æ–ª—å—à–µ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤
        ]
        
        # –°–∏—Å—Ç–µ–º–Ω—ã–π –ø—Ä–æ–º–ø—Ç –¥–ª—è –∞–≥–µ–Ω—Ç–∞
        self.system_prompt = ChatPromptTemplate.from_messages([
            ("system", """
            –¢—ã - –∫–æ—Ä–ø–æ—Ä–∞—Ç–∏–≤–Ω—ã–π –ø–æ–º–æ—â–Ω–∏–∫ –¥–ª—è –æ–Ω–±–æ—Ä–¥–∏–Ω–≥–∞ –Ω–æ–≤—ã—Ö —Å–æ—Ç—Ä—É–¥–Ω–∏–∫–æ–≤.
            –£ —Ç–µ–±—è –µ—Å—Ç—å –¥–æ—Å—Ç—É–ø –∫ —Å–ª–µ–¥—É—é—â–∏–º –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞–º:
            - user_search: –¥–ª—è –ø–æ–∏—Å–∫–∞ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ —Å–æ—Ç—Ä—É–¥–Ω–∏–∫–∞—Ö
            - document_search: –¥–ª—è –ø–æ–∏—Å–∫–∞ –∫–æ—Ä–ø–æ—Ä–∞—Ç–∏–≤–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
            
            –ò—Å–ø–æ–ª—å–∑—É–π —ç—Ç–∏ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã, –∫–æ–≥–¥–∞ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å —Å–ø—Ä–∞—à–∏–≤–∞–µ—Ç –æ:
            - –ö–æ–Ω—Ç–∞–∫—Ç–∞—Ö —Å–æ—Ç—Ä—É–¥–Ω–∏–∫–æ–≤
            - –ö–æ—Ä–ø–æ—Ä–∞—Ç–∏–≤–Ω—ã—Ö –ø—Ä–æ—Ü–µ–¥—É—Ä–∞—Ö –∏ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ö
            - –ü–æ–ª–∏—Ç–∏–∫–∞—Ö –∫–æ–º–ø–∞–Ω–∏–∏
            
            –í—Å–µ–≥–¥–∞ –±—É–¥—å –≤–µ–∂–ª–∏–≤—ã–º –∏ –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—ã–º.
            –ï—Å–ª–∏ –Ω–µ –º–æ–∂–µ—à—å –Ω–∞–π—Ç–∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é, —á–µ—Å—Ç–Ω–æ —Å–∫–∞–∂–∏ –æ–± —ç—Ç–æ–º.
            """),
            ("human", "{input}"),
            ("placeholder", "{agent_scratchpad}")
        ])
        
        # –°–æ–∑–¥–∞–Ω–∏–µ –∞–≥–µ–Ω—Ç–∞
        self.agent = create_openai_tools_agent(
            llm=self.llm,
            tools=self.tools,
            prompt=self.system_prompt
        )
        
        # –°–æ–∑–¥–∞–Ω–∏–µ executor
        self.agent_executor = AgentExecutor(
            agent=self.agent,
            tools=self.tools,
            verbose=True,
            max_iterations=3
        )
    
    async def process_query(self, query: str, user_id: int) -> str:
        """
        –û–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–ø—Ä–æ—Å–∞ —á–µ—Ä–µ–∑ –∞–≥–µ–Ω—Ç–∞ —Å –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞–º–∏
        """
        result = await self.agent_executor.ainvoke({
            "input": query,
            "user_id": user_id
        })
        
        return result["output"]
```

## –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Å Telegram Bot

### LangChain Integration in Bot Handlers
```python
from aiogram import Router, F
from aiogram.types import Message
from aiogram.filters import Command

class LangChainBotHandlers:
    def __init__(
        self,
        rag_workflow: LangGraphRAGWorkflow,
        conversational_rag: ConversationalRAGService,
        agent: CorporateAssistantAgent
    ):
        self.rag_workflow = rag_workflow
        self.conversational_rag = conversational_rag
        self.agent = agent
        self.router = Router()
        
        # –†–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏—è —Ö–µ–Ω–¥–ª–µ—Ä–æ–≤
        self.router.message(Command("search"))(self.search_command)
        self.router.message(Command("chat"))(self.chat_command)
        self.router.message(Command("agent"))(self.agent_command)
        self.router.message(F.text)(self.handle_text_message)
    
    async def search_command(self, message: Message, user: User):
        """–û–±—Ä–∞–±–æ—Ç–∫–∞ –∫–æ–º–∞–Ω–¥—ã –ø–æ–∏—Å–∫–∞ —á–µ—Ä–µ–∑ RAG"""
        query = message.text.replace("/search", "").strip()
        
        if not query:
            await message.answer("–ü–æ–∂–∞–ª—É–π—Å—Ç–∞, —É–∫–∞–∂–∏—Ç–µ –ø–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å –ø–æ—Å–ª–µ –∫–æ–º–∞–Ω–¥—ã /search")
            return
        
        # –û–±—Ä–∞–±–æ—Ç–∫–∞ —á–µ—Ä–µ–∑ LangGraph RAG workflow
        result = await self.rag_workflow.process_query(
            query=query,
            user_id=user.id
        )
        
        response_text = result["response"]
        
        # –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ–± –∏—Å—Ç–æ—á–Ω–∏–∫–∞—Ö
        if result["retrieved_docs_count"] > 0:
            response_text += f"\n\nüìö –ù–∞–π–¥–µ–Ω–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤: {result['retrieved_docs_count']}"
        
        await message.answer(response_text)
    
    async def chat_command(self, message: Message, user: User):
        """–û–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∏–∞–ª–æ–≥–æ–≤–æ–≥–æ —Ä–µ–∂–∏–º–∞"""
        query = message.text.replace("/chat", "").strip()
        
        if not query:
            await message.answer("–ù–∞—á–∏–Ω–∞–µ–º –¥–∏–∞–ª–æ–≥! –ó–∞–¥–∞–π—Ç–µ –≤–∞—à –≤–æ–ø—Ä–æ—Å.")
            return
        
        # –û–±—Ä–∞–±–æ—Ç–∫–∞ —á–µ—Ä–µ–∑ conversational RAG
        result = await self.conversational_rag.chat(
            query=query,
            user_id=user.id
        )
        
        await message.answer(result["answer"])
    